import argparse
import hail as hl
import hailtop.batch as hb
import hailtop.fs as hfs
import logging
import os

logging.basicConfig(level=logging.INFO, format='%(asctime)s: %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')


# Hail Batch currently has trouble processing more than 100k jobs at a time
MAX_HAIL_BATCH_JOBS_PER_RUN = 100_000

DOCKER_IMAGE = "hailgenetics/hail:0.2.119"


def parse_args():
    """Parse command line args."""
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-i", "--input-ht",
        help="Path of hail table generated by the previous step (rekey.py)",
        default="gs://gnomad-readviz/v4.0/gnomad.exomes.v4.0.readviz_crams_exploded_keyed_by_sample.ht",
    )
    parser.add_argument(
        "-o", "--output-bucket-path",
        help="Path where the .tsvs for each sample will be written",
        default="gs://gnomad-readviz/v4.0/readviz_tsvs",
    )
    parser.add_argument(
        "-f", "--overwrite-existing-tsvs",
        action="store_true",
        help="Overwrite/update existing TSV checkpoints.",
    )
    parser.add_argument(
        "-s", "--start-with-sample-i",
        help="0-based offset into the list of sample ids.",
        type=int,
        default=0,
    )
    parser.add_argument(
        "-n", "--n-samples-to-process",
        help="Process at most this many sample ids.",
        type=int,
        default=MAX_HAIL_BATCH_JOBS_PER_RUN,
    )
    parser.add_argument(
        "--tmp-bucket",
        help="Bucket for intermediate files.",
        default="gs://bw2-delete-after-5-days",
    )
    parser.add_argument(
        "--billing-project",
        help="Billing project to use with hail Batch.",
        default="broad-mpg-gnomad",
    )
    parser.add_argument(
        "sample_ids_path",
        help="A text file containing one sample id per line. If this file doesn't exist, it will be created.",
    )
    args = parser.parse_args()

    return args


def export_per_sample_tsv(input_ht_path, sample_id, tsv_output_path):
    """Iterate over sample_ids and export all records with that sample id to separate tsv(s).

    Args:
        input_ht_path (str): path of output hail table from step2__rekey.py
        sample_id (str): sample id to export
        tsv_output_path (str): path where to write the .tsv file for this sample
    """

    ht = hl.read_table(input_ht_path)
    per_sample_ht = ht.filter(ht.S == sample_id, keep=True)
    per_sample_ht = per_sample_ht.key_by()
    # re-order columns and drop sample id since it's in the .tsv filename
    per_sample_ht = per_sample_ht.select('chrom', 'pos', 'ref', 'alt', 'het_or_hom_or_hemi', 'GQ')
    per_sample_ht.export(tsv_output_path, header=False)
    logging.info(f"Done exporting {per_sample_ht.count():,d} samples to {tsv_output_path}")


def main():
    args = parse_args()

    sample_ids = []
    with hfs.open(args.sample_ids_path) if args.sample_ids_path.startswith("gs://" ) else open(args.sample_ids_path, "rt") as f:
        for i, line in enumerate(f):
            sample_id = line.rstrip("\n")
            sample_ids.append(sample_id)

    logging.info(f"Parsed {len(sample_ids):,d} sample ids from {args.sample_ids_path}")

    # check for existing TSVs
    if not args.overwrite_existing_tsvs:
        existing_paths = hfs.ls(args.output_bucket_path)
        already_processed_sample_ids = set([os.path.basename(e.path).replace(".tsv.bgz", "") for e in existing_paths])
        if len(already_processed_sample_ids) > 0:
            logging.info(f"Found {len(already_processed_sample_ids):,d} existing TSVs in {args.output_bucket_path}")
            sample_ids = sorted(set(sample_ids) - set(already_processed_sample_ids))
            logging.info(f"Will process {len(sample_ids):,d} sample ids that haven't been processed yet")

    # filter sample ids
    if args.start_with_sample_i:
        sample_ids = sample_ids[args.start_with_sample_i:]
    if args.n_samples_to_process:
        sample_ids = sample_ids[:min(args.n_samples_to_process, MAX_HAIL_BATCH_JOBS_PER_RUN)]
    if len(sample_ids) == 0:
        logging.info("No samples to process. Exiting.")
        return

    # create jobs
    backend = hb.ServiceBackend(billing_project=args.billing_project, remote_tmpdir=args.tmp_bucket)
    b = hb.Batch("gnomad-readviz step3: export per-sample TSVs", backend=backend)
    for i, sample_id in enumerate(sample_ids):
        sample_id = sample_id.replace("/", "_")
        tsv_output_path = os.path.join(args.output_bucket_path, f"{sample_id}.tsv.bgz")
        j = b.new_python_job(sample_id)
        j.image(DOCKER_IMAGE)
        j.cpu(0.25)
        j.call(export_per_sample_tsv, args.input_ht, sample_id, tsv_output_path)
    b.run()


if __name__ == "__main__":
    main()
